!pip install torch
!pip install transformers

import pandas as pd
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

QA_input = [
    {'question': 'Why is conversion important?',
     'context': 'The Options to convert models between FARM and transformers gives freedom to users and between Frameworks'},
    {'question': 'What is your name?',
     'context': 'My name is Anthony. Mein Dunya mein Ekala hu'}
]


model_name = 'deepset/roberta-base-squad2'

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

inputs0 = tokenizer(QA_input[0]['question'], QA_input[0]['context'], return_tensors="pt")
outputs0 = model(**inputs0)

inputs1 = tokenizer(QA_input[1]['question'], QA_input[1]['context'], return_tensors="pt")
outputs1 = model(**inputs1)

answer_start_idx = torch.argmax(outputs0.start_logits)
answer_end_idx = torch.argmax(outputs0.end_logits)

answer_tokens = inputs0.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)
print('ques: {}\nanswer: {}'.format(QA_input[0]['question'], answer))

answer_start_idx = torch.argmax(outputs1.start_logits)
answer_end_idx = torch.argmax(outputs1.end_logits)

answer_tokens = inputs1.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)
print('ques: {}\nanswer: {}'.format(QA_input[1]['question'], answer))