Alternative approaches:

Pre-trained Q&A models: Consider pre-trained models like Google's T5-QnA or Facebook's BART-QA specifically designed for question answering tasks. These models might be easier to fine-tune and require less training data than LLaMA.
Hybrid approach: Combine keyword matching with a smaller LLaMA model trained on your specific question-answer dataset. This offers a balance between efficiency and the potential for handling complex questions.